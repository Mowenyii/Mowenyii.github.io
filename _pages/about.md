---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<!-- I'm a in-coming phd student at Rutgers University, where I am advised by Prof. [Dimitris N. Metaxas](https://people.cs.rutgers.edu/~dnm/) I got master degree at [Renmin University of China](http://ai.ruc.edu.cn/english/index.htm), where I am advised by [Bing Su](https://gsai.ruc.edu.cn/english/bingsu). I earned my B.E. from South China University of Technology. My research focuses on multimodal learning, particularly enhancing user alignment and image controllability in generative models. Recently, Iâ€™ve worked on autoregressive and diffusion models, exploring applications like text-to-image generation and preference alignment. You can view my CV [here](https://mowenyii.github.io/files/Wenyi_Mo.pdf). -->

<!-- > I am always open for research discussions and collaborations :). Also, I am looking for a potential Ph.D. position enrolling in Fall 2025. Welcome to reach out to me if interested. -->

<!-- , and earned my B.E. from the South China University of Technology -->

I am a first-year Ph.D. student at [Rutgers University](https://www.rutgers.edu/), where I am advised by [Prof. Dimitris N. Metaxas](https://people.cs.rutgers.edu/~dnm/). I received my Masterâ€™s degree from the [Renmin University of China](http://ai.ruc.edu.cn/english/index.htm), advised by [Prof. Bing Su](https://gsai.ruc.edu.cn/english/bingsu).
My research interests lie in multimodal learning, with a focus on enhancing user alignment and image controllability in generative models. Recently, Iâ€™ve been working with autoregressive and diffusion models, particularly in the context of text-to-image generation and preference alignment.
<!-- You can find my CV [here](https://mowenyii.github.io/files/Wenyi_Mo.pdf). -->





# ğŸ”¥ News
- *06/2025*: ğŸ‰ [One co-authored paper](https://github.com/BarretBa/ICTHP) about Text-to-Image generation accepted to ICCV 2025.
- *01/2025*: ğŸ‰ One co-authored paper about vision-language models accepted to NeuralÂ Networks 2025.
- *11/2024*: ğŸ‰ Recognized as a [top reviewer at NeurIPS 2024](https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers).
- *10/2024*: ğŸ‰ One first-authored paper about image editing accepted to WACV 2025.
- *02/2024*: ğŸ‰ One first-authored paper about Text-to-Image generation accepted to CVPR 2024.
- *09/2022*: ğŸ‰ One co-authored paper accepted to NeurIPS 2022.



# ğŸ“ Publications 






<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/DF_example.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Dynamic Prompt Optimizing for Text-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_Dynamic_Prompt_Optimizing_for_Text-to-Image_Generation_CVPR_2024_paper.pdf)

**Wenyi Mo**, Tianyu Zhang, Yalong Bai, Bing Su<sup>âœ‰</sup>, Ji-Rong Wen, Qing Yang. 

<span style="color:red">(CVPR 2024)</span>  [![](https://img.shields.io/github/stars/Mowenyii/PAE?style=social&label=Code+Stars)](https://github.com/Mowenyii/PAE)

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/ICCV25-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment](https://arxiv.org/abs/2507.19002)

Ying Ba, Tianyu Zhang, Yalong Bai, **Wenyi Mo**, Tao Liang, Bing Su, Ji-Rong Wen. 

<span style="color:red">(ICCV 2025)</span>  [![](https://img.shields.io/github/stars/BarretBa/ICTHP?style=social&label=Code+Stars)](https://github.com/BarretBa/ICTHP) 


</div>
</div>


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='images/under_review.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[User-Specific Preference Prediction on Generated Images](https://mowenyii.github.io/files/25_wenyi_arxiv.pdf)

**Wenyi Mo**, Tianyu Zhang, Yalong Bai, Jieqiong Liu, Bing Su<sup>âœ‰</sup>, Biye Li, Ji-Rong Wen. 

</div>
</div> -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">WACV 2025</div><img src='images/wacv.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and Editing](https://arxiv.org/abs/2411.19652)

**Wenyi Mo**, Tianyu Zhang, Yalong Bai, Bing Su<sup>âœ‰</sup>, Ji-Rong Wen. 

<span style="color:red">(WACV 2025)</span>  [![](https://img.shields.io/github/stars/Mowenyii/Uniform-Attention-Maps?style=social&label=Code+Stars)](https://github.com/Mowenyii/Uniform-Attention-Maps) 


</div>
</div>

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">NN 2025</div><img src='images/nn25_CPKP.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Supporting Vision-Language Model Few-Shot Inference with Confounder-Pruned knowledge Prompt](https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=4909583)

Jiangmeng Li, **Wenyi Mo**, Fei Song, Chuxiong Sun, Wenwen Qiang, Bing Su, and Changwen Zheng. 

<span style="color:red">(NeuralÂ Networks 2025)</span>  [![](https://img.shields.io/github/stars/Mowenyii/CPKP?style=social&label=Code+Stars)](https://github.com/Mowenyii/CPKP) 


</div>
</div> -->



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/neurips22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning](https://papers.neurips.cc/paper_files/paper/2022/file/fb575ab4d882a4c734641155a5f30911-Paper-Conference.pdf)

 Jiangmeng Li\*, Wenwen Qiang\*, Yanan Zhang, **Wenyi Mo**, Changwen Zheng, Bing Su<sup>âœ‰</sup>, and Hui Xiong. 
 
  <span style="color:red">(NeurIPS 2022, Spotlight)</span>  [![](https://img.shields.io/github/stars/jiangmengli/metamask?style=social&label=Code+Stars)](https://github.com/jiangmengli/metamask)

</div>
</div>


# ğŸ‘©â€ğŸ« Invited Talks
- *10/2025*, Poster Presentation @ ICCV Honolulu, HawaiÊ»i. 
- *06/2024*, Poster Presentation @ CVPR Seattle, Washington. 

# ğŸ– Honors and Awards
- *10/2024*, Merit Graduated Student of Renmin University of China.
- *10/2024*, Renmin University of China Scholarship. 
- *12/2021*, China National Encouragement Scholarship, (Top 3%). 
- *12/2019*, China National Scholarship, (Top 1%). 


# ğŸ‘©â€ğŸ’» Academic Services
- Conference Reviewer: NeurIPS (2024, 2025), ICLR (2025, 2026), AISTATS (2025, 2026), CVPR (2025, 2026), WACV 2025,  ICML 2025, ICCV 2025, AAAI 2026.
<!-- - Journal Reviewer: Scientific Reports -->

<!-- # ğŸ“– Educations
- *08/2022 - 06/2025*, Master, Department of  Artificial Intelligence, Renmin University of China, Beijing.
- *08/2018 - 06/2022*, Undergraduate, Department of Computer Science, South China University of Technology, Guangzhou. -->



# ğŸ’» Internships
- *03/2024 - 09/2025*, In2X, Beijing, China.
- *01/2024 - 03/2024*, Bytedance Seed, Shanghai, China.
- *09/2023 - 01/2024*, Du Xiaoman Technology, Beijing, China.
- *01/2022 - 03/2022*, Jingdong Exploration Research Institute, Beijing, China.


<link rel="stylesheet" href="/assets/css/gallery.css">

<span class='anchor' id='gallery'></span>

# ğŸ“¸ Conference Gallery

Here are some photos from conferences and academic events I've attended:

<div class="photo-gallery">
  <div class="gallery-container">
    <div class="gallery-slider">
      <!-- CVPR 2024 ä¼šè®®ç…§ç‰‡ -->
      <div class="gallery-item">
        <img src="images/cvpr_2024.jpg" alt="CVPR 2024 Conference" />
        <div class="gallery-caption">
          <h3>CVPR 2024</h3>
          <p>Seattle, Washington - CVPR 2024</p>
        </div>
      </div>
      <!-- ICCV 2025 ä¼šè®®ç…§ç‰‡ -->
      <div class="gallery-item">
        <img src="images/iccv_2025.jpg" alt="ICCV 2025 Conference" />
        <div class="gallery-caption">
          <h3>ICCV 2025</h3>
          <p>Honolulu, HawaiÊ»i - ICCV 2025</p>
        </div>
      </div>
      <!-- ä¸ªäººç…§ç‰‡
      <div class="gallery-item">
        <img src="images/me.jpg" alt="Personal Photo" />
        <div class="gallery-caption">
          <h3>å­¦æœ¯ç ”ç©¶</h3>
          <p>ä¸“æ³¨äºç”Ÿæˆæ¨¡å‹å’Œå¤šæ¨¡æ€å­¦ä¹ ç ”ç©¶</p>
        </div>
      </div> -->
    </div>
    <!-- å¯¼èˆªæŒ‰é’® -->
    <button class="gallery-nav prev" onclick="changeSlide(-1)">â®</button>
    <button class="gallery-nav next" onclick="changeSlide(1)">â¯</button>
    <!-- æŒ‡ç¤ºå™¨ -->
    <div class="gallery-indicators">
      <span class="indicator active" onclick="currentSlide(1)"></span>
      <span class="indicator" onclick="currentSlide(2)"></span>
      <!-- <span class="indicator" onclick="currentSlide(3)"></span>
      <span class="indicator" onclick="currentSlide(4)"></span>
      <span class="indicator" onclick="currentSlide(5)"></span> -->
    </div>
  </div>
</div>


<script src="/assets/js/gallery.js"></script>

<!-- # ğŸšŒ Visitor Map
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=1nRIFNSwVclwApwJhz16pZZAjy8G47awU92Eq0b1mQ8"></script> -->


### ğŸ¨ Hobbies

Outside research, I enjoy exploring art, film, and music as ways to understand human creativity from different perspectives. I love hiking in places such as [Ross Mountain](https://www.oregonhikers.org/field_guide/Ross_Mountain), [Taihang Mountain](https://en.wikipedia.org/wiki/Taihang_Mountains), [the Great Wall](https://en.wikipedia.org/wiki/Great_Wall_of_China), and [Diamond Head State Monument](https://dlnr.hawaii.gov/dsp/parks/oahu/diamond-head-state-monument/).
Some of my favorite films include [*Coraline*](https://www.imdb.com/title/tt0327597/) for its enchanting stop-motion artistry and [*The Shining*](https://www.imdb.com/title/tt0081505/) for its psychological depth.
Musically, I often listen to [David Tao](https://en.wikipedia.org/wiki/David_Tao), [Utada Hikaru](https://en.wikipedia.org/wiki/Utada_Hikaru), [Erik Satie](https://en.wikipedia.org/wiki/Erik_Satie), and [Ryuichi Sakamoto](https://en.wikipedia.org/wiki/Ryuichi_Sakamoto).
In visual arts, I particularly appreciate the works of [Suzanne Valadon](https://en.wikipedia.org/wiki/Suzanne_Valadon), [Edward Hopper](https://en.wikipedia.org/wiki/Edward_Hopper), [Vincent van Gogh](https://en.wikipedia.org/wiki/Vincent_van_Gogh), and [Caravaggio](https://en.wikipedia.org/wiki/Caravaggio).