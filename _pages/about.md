---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<!-- I'm a in-coming phd student at Rutgers University, where I am advised by Prof. [Dimitris N. Metaxas](https://people.cs.rutgers.edu/~dnm/) I got master degree at [Renmin University of China](http://ai.ruc.edu.cn/english/index.htm), where I am advised by [Bing Su](https://gsai.ruc.edu.cn/english/bingsu). I earned my B.E. from South China University of Technology. My research focuses on multimodal learning, particularly enhancing user alignment and image controllability in generative models. Recently, I‚Äôve worked on autoregressive and diffusion models, exploring applications like text-to-image generation and preference alignment. You can view my CV [here](https://mowenyii.github.io/files/Wenyi_Mo.pdf). -->

<!-- > I am always open for research discussions and collaborations :). Also, I am looking for a potential Ph.D. position enrolling in Fall 2025. Welcome to reach out to me if interested. -->

<!-- , and earned my B.E. from the South China University of Technology -->

I am a first-year Ph.D. student at [Rutgers University](https://www.rutgers.edu/), where I am advised by [Prof. Dimitris N. Metaxas](https://people.cs.rutgers.edu/~dnm/). I received my Master‚Äôs degree from the [Renmin University of China](http://ai.ruc.edu.cn/english/index.htm), advised by [Prof. Bing Su](https://gsai.ruc.edu.cn/english/bingsu).
My research interests lie in multimodal learning, with a focus on enhancing user alignment and image controllability in generative models. Recently, I‚Äôve been working with autoregressive and diffusion models, particularly in the context of text-to-image generation and preference alignment.
<!-- You can find my CV [here](https://mowenyii.github.io/files/Wenyi_Mo.pdf). -->





# üî• News
- *06/2025*: üéâ [One co-authored paper](https://github.com/BarretBa/ICTHP) about Text-to-Image generation accepted to ICCV 2025.
- *01/2025*: üéâ One co-authored paper about vision-language models accepted to Neural¬†Networks 2025.
- *11/2024*: üéâ Recognized as a [top reviewer at NeurIPS 2024](https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers).
- *10/2024*: üéâ One first-authored paper about image editing accepted to WACV 2025.
- *02/2024*: üéâ One first-authored paper about Text-to-Image generation accepted to CVPR 2024.
- *09/2022*: üéâ One co-authored paper accepted to NeurIPS 2022.



# üìù Publications 






<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/DF_example.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Dynamic Prompt Optimizing for Text-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_Dynamic_Prompt_Optimizing_for_Text-to-Image_Generation_CVPR_2024_paper.pdf)

**Wenyi Mo**, Tianyu Zhang, Yalong Bai, Bing Su<sup>‚úâ</sup>, Ji-Rong Wen, Qing Yang. 

<span style="color:red">(CVPR 2024)</span>  [![](https://img.shields.io/github/stars/Mowenyii/PAE?style=social&label=Code+Stars)](https://github.com/Mowenyii/PAE)

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/ICCV25-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment](https://arxiv.org/abs/2507.19002)

Ying Ba, Tianyu Zhang, Yalong Bai, **Wenyi Mo**, Tao Liang, Bing Su, Ji-Rong Wen. 

<span style="color:red">(ICCV 2025)</span>  [![](https://img.shields.io/github/stars/BarretBa/ICTHP?style=social&label=Code+Stars)](https://github.com/BarretBa/ICTHP) 


</div>
</div>


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='images/under_review.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[User-Specific Preference Prediction on Generated Images](https://mowenyii.github.io/files/25_wenyi_arxiv.pdf)

**Wenyi Mo**, Tianyu Zhang, Yalong Bai, Jieqiong Liu, Bing Su<sup>‚úâ</sup>, Biye Li, Ji-Rong Wen. 

</div>
</div> -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">WACV 2025</div><img src='images/wacv.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and Editing](https://arxiv.org/abs/2411.19652)

**Wenyi Mo**, Tianyu Zhang, Yalong Bai, Bing Su<sup>‚úâ</sup>, Ji-Rong Wen. 

<span style="color:red">(WACV 2025)</span>  [![](https://img.shields.io/github/stars/Mowenyii/Uniform-Attention-Maps?style=social&label=Code+Stars)](https://github.com/Mowenyii/Uniform-Attention-Maps) 


</div>
</div>

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">NN 2025</div><img src='images/nn25_CPKP.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Supporting Vision-Language Model Few-Shot Inference with Confounder-Pruned knowledge Prompt](https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=4909583)

Jiangmeng Li, **Wenyi Mo**, Fei Song, Chuxiong Sun, Wenwen Qiang, Bing Su, and Changwen Zheng. 

<span style="color:red">(Neural¬†Networks 2025)</span>  [![](https://img.shields.io/github/stars/Mowenyii/CPKP?style=social&label=Code+Stars)](https://github.com/Mowenyii/CPKP) 


</div>
</div> -->



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/neurips22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning](https://papers.neurips.cc/paper_files/paper/2022/file/fb575ab4d882a4c734641155a5f30911-Paper-Conference.pdf)

 Jiangmeng Li\*, Wenwen Qiang\*, Yanan Zhang, **Wenyi Mo**, Changwen Zheng, Bing Su<sup>‚úâ</sup>, and Hui Xiong. 
 
  <span style="color:red">(NeurIPS 2022, Spotlight)</span>  [![](https://img.shields.io/github/stars/jiangmengli/metamask?style=social&label=Code+Stars)](https://github.com/jiangmengli/metamask)

</div>
</div>


# üë©‚Äçüè´ Invited Talks
- *10/2025*, Poster Presentation @ ICCV Honolulu, Hawai ªi. 
- *06/2024*, Poster Presentation @ CVPR Seattle, Washington. 

# üéñ Honors and Awards
- *10/2024*, Merit Graduated Student of Renmin University of China.
- *10/2024*, Renmin University of China Scholarship. 
- *12/2021*, China National Encouragement Scholarship, (Top 3%). 
- *12/2019*, China National Scholarship, (Top 1%). 


# üë©‚Äçüíª Academic Services
- Conference Reviewer: NeurIPS ([2024](https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers), [2025](https://neurips.cc/Conferences/2025/ProgramCommittee)), ICLR ([2025](https://iclr.cc/Conferences/2025/ProgramCommittee), 2026), AISTATS (2025, 2026), CVPR ([2025](https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee), 2026), [WACV 2025](https://csdl-downloads.ieeecomputer.org/proceedings/wacv/2025/1083/00/108300z185.pdf?Expires=1761594373&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jc2RsLWRvd25sb2Fkcy5pZWVlY29tcHV0ZXIub3JnL3Byb2NlZWRpbmdzL3dhY3YvMjAyNS8xMDgzLzAwLzEwODMwMHoxODUucGRmIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzYxNTk0MzczfX19XX0_&Signature=xWEIEU8VRX6B9EVfW2msdOVeJCor35fOWCReEAQgmqinXwyXbWapMGLXmzOslDvW~eHi93q5wfZOjQcb6gm0EyuwAsw-RA5qaQD57syMdQFwd6xYmsN7mt-REJZquBS45c670Ldu3FNFwAOR92xqPyIdRLuObZ~70pwQjEFNkD2SEmDCijJCh9ZVu5fDvh72eca5ax9TbUMj1LExVRBoB295HS5KJasYT2rQFVI740JUsRSaMxunfERTObJ7DRSb92qFA1q~EvmChTSPQmlYyTIUXGcxbHR3VYkwjBmPdncKjkK3FSV2WFos55jaPJrWNEUTp6x8PVFTJ-mK-4Ph~g__&Key-Pair-Id=K12PMWTCQBDMDT),  [ICML 2025](https://icml.cc/Conferences/2025/ProgramCommittee), [ICCV 2025](https://iccv.thecvf.com/Conferences/2025/ProgramCommittee), AAAI 2026.
- Journal Reviewer: International Journal of Image and Graphics, Scientific Reports.

<!-- # üìñ Educations
- *08/2022 - 06/2025*, Master, Department of  Artificial Intelligence, Renmin University of China, Beijing.
- *08/2018 - 06/2022*, Undergraduate, Department of Computer Science, South China University of Technology, Guangzhou. -->



# üíª Internships
- *03/2024 - 09/2025*, In2X, Beijing, China.
- *01/2024 - 03/2024*, Bytedance Seed, Shanghai, China.
- *09/2023 - 01/2024*, Du Xiaoman Technology, Beijing, China.
- *01/2022 - 03/2022*, Jingdong Exploration Research Institute, Beijing, China.




# üé® Hobbies

Outside research, I enjoy exploring art, film, and music as ways to understand human creativity from different perspectives. I love hiking in places such as [Taihang Mountain](https://en.wikipedia.org/wiki/Taihang_Mountains) (Henan, China), [the Great Wall](https://en.wikipedia.org/wiki/Great_Wall_of_China) (Beijing, China), [Enshi Grand Canyon](https://en.wikipedia.org/wiki/Enshi_Grand_Canyon) (Hubei, China), and [Diamond Head State Monument](https://dlnr.hawaii.gov/dsp/parks/oahu/diamond-head-state-monument/) (Honolulu, Hawaii).
Some of my favorite films include [*Coraline*](https://www.imdb.com/title/tt0327597/) for its enchanting stop-motion artistry and [*The Shining*](https://www.imdb.com/title/tt0081505/) for its psychological depth.
Musically, I often listen to [David Tao](https://en.wikipedia.org/wiki/David_Tao), [Utada Hikaru](https://en.wikipedia.org/wiki/Utada_Hikaru), [Erik Satie](https://historia-arte.com/obras/un-bohemio-erik-satie-en-su-estudio), and [Ryuichi Sakamoto](https://en.wikipedia.org/wiki/Ryuichi_Sakamoto).
In visual arts, I particularly appreciate the works of [Suzanne Valadon](https://en.wikipedia.org/wiki/Suzanne_Valadon), [Edward Hopper](https://historia-arte.com/artistas/edward-hopper), [Vincent van Gogh](https://en.wikipedia.org/wiki/Vincent_van_Gogh), and [Caravaggio](https://en.wikipedia.org/wiki/Caravaggio).


<link rel="stylesheet" href="/assets/css/gallery.css">

<span class='anchor' id='gallery'></span>

# üì∏ Conference Gallery

Here are some photos from conferences and academic events I've attended:

<div class="photo-gallery">
  <div class="gallery-container">
    <div class="gallery-slider">
      <!-- CVPR 2024 ‰ºöËÆÆÁÖßÁâá -->
      <div class="gallery-item">
        <img src="images/cvpr_2024.jpg" alt="CVPR 2024 Conference" />
        <div class="gallery-caption">
          <h3>CVPR 2024</h3>
          <p>Seattle, Washington - CVPR 2024</p>
        </div>
      </div>
      <!-- ICCV 2025 ‰ºöËÆÆÁÖßÁâá -->
      <div class="gallery-item">
        <img src="images/iccv_2025.jpg" alt="ICCV 2025 Conference" />
        <div class="gallery-caption">
          <h3>ICCV 2025</h3>
          <p>Honolulu, Hawai ªi - ICCV 2025</p>
        </div>
      </div>
      <!-- ‰∏™‰∫∫ÁÖßÁâá
      <div class="gallery-item">
        <img src="images/me.jpg" alt="Personal Photo" />
        <div class="gallery-caption">
          <h3>Â≠¶ÊúØÁ†îÁ©∂</h3>
          <p>‰∏ìÊ≥®‰∫éÁîüÊàêÊ®°ÂûãÂíåÂ§öÊ®°ÊÄÅÂ≠¶‰π†Á†îÁ©∂</p>
        </div>
      </div> -->
    </div>
    <!-- ÂØºËà™ÊåâÈíÆ -->
    <button class="gallery-nav prev" onclick="changeSlide(-1)">‚ùÆ</button>
    <button class="gallery-nav next" onclick="changeSlide(1)">‚ùØ</button>
    <!-- ÊåáÁ§∫Âô® -->
    <div class="gallery-indicators">
      <span class="indicator active" onclick="currentSlide(1)"></span>
      <span class="indicator" onclick="currentSlide(2)"></span>
      <!-- <span class="indicator" onclick="currentSlide(3)"></span>
      <span class="indicator" onclick="currentSlide(4)"></span>
      <span class="indicator" onclick="currentSlide(5)"></span> -->
    </div>
  </div>
</div>


<script src="/assets/js/gallery.js"></script>



<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=1nRIFNSwVclwApwJhz16pZZAjy8G47awU92Eq0b1mQ8"></script>
