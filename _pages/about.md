---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<!-- I'm a in-coming phd student at Rutgers University, where I am advised by Prof. [Dimitris N. Metaxas](https://people.cs.rutgers.edu/~dnm/) I got master degree at [Renmin University of China](http://ai.ruc.edu.cn/english/index.htm), where I am advised by [Bing Su](https://gsai.ruc.edu.cn/english/bingsu). I earned my B.E. from South China University of Technology. My research focuses on multimodal learning, particularly enhancing user alignment and image controllability in generative models. Recently, I’ve worked on autoregressive and diffusion models, exploring applications like text-to-image generation and preference alignment. You can view my CV [here](https://mowenyii.github.io/files/Wenyi_Mo.pdf). -->

<!-- > I am always open for research discussions and collaborations :). Also, I am looking for a potential Ph.D. position enrolling in Fall 2025. Welcome to reach out to me if interested. -->

<!-- , and earned my B.E. from the South China University of Technology -->

I am a first-year Ph.D. student at [Rutgers University](https://www.rutgers.edu/), where I am advised by [Prof. Dimitris N. Metaxas](https://people.cs.rutgers.edu/~dnm/). I received my Master’s degree from the [Renmin University of China](http://ai.ruc.edu.cn/english/index.htm), advised by [Prof. Bing Su](https://gsai.ruc.edu.cn/english/bingsu).
My research interests lie in multimodal learning, with a focus on enhancing user alignment and image controllability in generative models. Recently, I’ve been working with autoregressive and diffusion models, particularly in the context of text-to-image generation and preference alignment.
<!-- You can find my CV [here](https://mowenyii.github.io/files/Wenyi_Mo.pdf). -->





# 🔥 News
- *06/2025*: 🎉 [One co-authored paper](https://github.com/BarretBa/ICTHP) about Text-to-Image generation accepted to ICCV 2025.
- *01/2025*: 🎉 One co-authored paper about vision-language models accepted to Neural Networks 2025.
- *11/2024*: 🎉 Recognized as a [top reviewer at NeurIPS 2024](https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers).
- *10/2024*: 🎉 One first-authored paper about image editing accepted to WACV 2025.
- *02/2024*: 🎉 One first-authored paper about Text-to-Image generation accepted to CVPR 2024.
- *09/2022*: 🎉 One co-authored paper accepted to NeurIPS 2022.



# 📝 Publications 






<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/DF_example.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Dynamic Prompt Optimizing for Text-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_Dynamic_Prompt_Optimizing_for_Text-to-Image_Generation_CVPR_2024_paper.pdf)

**Wenyi Mo**, Tianyu Zhang, Yalong Bai, Bing Su<sup>✉</sup>, Ji-Rong Wen, Qing Yang. 

<span style="color:red">(CVPR 2024)</span>  [![](https://img.shields.io/github/stars/Mowenyii/PAE?style=social&label=Code+Stars)](https://github.com/Mowenyii/PAE)

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/ICCV25-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment](https://arxiv.org/abs/2507.19002)

Ying Ba, Tianyu Zhang, Yalong Bai, **Wenyi Mo**, Tao Liang, Bing Su, Ji-Rong Wen. 

<span style="color:red">(ICCV 2025)</span>  [![](https://img.shields.io/github/stars/BarretBa/ICTHP?style=social&label=Code+Stars)](https://github.com/BarretBa/ICTHP) 


</div>
</div>


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='images/under_review.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[User-Specific Preference Prediction on Generated Images](https://mowenyii.github.io/files/25_wenyi_arxiv.pdf)

**Wenyi Mo**, Tianyu Zhang, Yalong Bai, Jieqiong Liu, Bing Su<sup>✉</sup>, Biye Li, Ji-Rong Wen. 

</div>
</div> -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">WACV 2025</div><img src='images/wacv.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and Editing](https://arxiv.org/abs/2411.19652)

**Wenyi Mo**, Tianyu Zhang, Yalong Bai, Bing Su<sup>✉</sup>, Ji-Rong Wen. 

<span style="color:red">(WACV 2025)</span>  [![](https://img.shields.io/github/stars/Mowenyii/Uniform-Attention-Maps?style=social&label=Code+Stars)](https://github.com/Mowenyii/Uniform-Attention-Maps) 


</div>
</div>

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">NN 2025</div><img src='images/nn25_CPKP.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Supporting Vision-Language Model Few-Shot Inference with Confounder-Pruned knowledge Prompt](https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=4909583)

Jiangmeng Li, **Wenyi Mo**, Fei Song, Chuxiong Sun, Wenwen Qiang, Bing Su, and Changwen Zheng. 

<span style="color:red">(Neural Networks 2025)</span>  [![](https://img.shields.io/github/stars/Mowenyii/CPKP?style=social&label=Code+Stars)](https://github.com/Mowenyii/CPKP) 


</div>
</div> -->



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/neurips22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning](https://papers.neurips.cc/paper_files/paper/2022/file/fb575ab4d882a4c734641155a5f30911-Paper-Conference.pdf)

 Jiangmeng Li\*, Wenwen Qiang\*, Yanan Zhang, **Wenyi Mo**, Changwen Zheng, Bing Su<sup>✉</sup>, and Hui Xiong. 
 
  <span style="color:red">(NeurIPS 2022, Spotlight)</span>  [![](https://img.shields.io/github/stars/jiangmengli/metamask?style=social&label=Code+Stars)](https://github.com/jiangmengli/metamask)

</div>
</div>


# 👩‍🏫 Invited Talks
- *10/2025*, Poster Presentation @ ICCV Honolulu, Hawaiʻi. 
- *06/2024*, Poster Presentation @ CVPR Seattle, Washington. 

# 🎖 Honors and Awards
- *10/2024*, Merit Graduated Student of Renmin University of China.
- *10/2024*, Renmin University of China Scholarship. 
- *12/2021*, China National Encouragement Scholarship, (Top 3%). 
- *12/2019*, China National Scholarship, (Top 1%). 


# 👩‍💻 Academic Services
- Conference Reviewer: NeurIPS (2024, 2025), ICLR (2025, 2026), AISTATS (2025, 2026), CVPR (2025, 2026), WACV 2025,  ICML 2025, ICCV 2025, AAAI 2026.
<!-- - Journal Reviewer: Scientific Reports -->

<!-- # 📖 Educations
- *08/2022 - 06/2025*, Master, Department of  Artificial Intelligence, Renmin University of China, Beijing.
- *08/2018 - 06/2022*, Undergraduate, Department of Computer Science, South China University of Technology, Guangzhou. -->



# 💻 Internships
- *03/2024 - 09/2025*, In2X, Beijing, China.
- *01/2024 - 03/2024*, Bytedance Seed, Shanghai, China.
- *09/2023 - 01/2024*, Du Xiaoman Technology, Beijing, China.
- *01/2022 - 03/2022*, Jingdong Exploration Research Institute, Beijing, China.

<!-- # 🚌 Visitor Map -->
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=1nRIFNSwVclwApwJhz16pZZAjy8G47awU92Eq0b1mQ8"></script>
